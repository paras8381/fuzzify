# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VJOBaMqs7VTpy8O-DCtFVBe-MrXUKURK
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load Node 1 dataset
df = pd.read_csv("/content/smartgrid_node1.csv")

# Basic statistics
print("Dataset Summary:\n", df.describe())

# Plot feature distributions
plt.figure(figsize=(12, 6))
for i, col in enumerate(df.columns[:-1]):
    plt.subplot(2, 3, i+1)
    sns.histplot(df[col], kde=True, bins=20, color='skyblue')
    plt.title(f"{col.capitalize()} Distribution")
plt.tight_layout()
plt.show()

# Plot fault label distribution
plt.figure(figsize=(4, 3))
sns.countplot(x='fault', data=df, palette='Set2')
plt.title("Fault Label Distribution")
plt.xlabel("Fault (0 = No, 1 = Yes)")
plt.ylabel("Count")
plt.show()

!pip install scikit-fuzzy
!pip install scikit-learn

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Load and split data
df = pd.read_csv("/content/smartgrid_node1.csv")
X = df.drop('fault', axis=1).values
y = df['fault'].values

# Normalize features
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print("Training data shape:", X_train.shape)
print("Test data shape:", X_test.shape)

import skfuzzy as fuzz

def fuzzify_feature(x, feature_range):
    """
    Apply triangular membership functions: low, medium, high.
    Returns fuzzy vector [low_membership, medium_membership, high_membership]
    """
    low = fuzz.trimf(x, [feature_range[0], feature_range[0], feature_range[1]])
    medium = fuzz.trimf(x, [feature_range[0], feature_range[1], feature_range[2]])
    high = fuzz.trimf(x, [feature_range[1], feature_range[2], feature_range[2]])
    return np.vstack((low, medium, high)).T

# Example: fuzzify one feature (voltage) in training set
voltage_range = [0, 0.5, 1]  # Since we scaled to 0â€“1
voltage_fuzzy = fuzzify_feature(X_train[:, 0], voltage_range)

print("Fuzzy-transformed voltage shape:", voltage_fuzzy.shape)

import skfuzzy as fuzz
import numpy as np

def fuzzify_feature_column(column_data, feature_range):
    """
    Apply triangular membership functions (low, medium, high) to a single feature column.
    """
    low = fuzz.trimf(column_data, [feature_range[0], feature_range[0], feature_range[1]])
    medium = fuzz.trimf(column_data, [feature_range[0], feature_range[1], feature_range[2]])
    high = fuzz.trimf(column_data, [feature_range[1], feature_range[2], feature_range[2]])
    return np.vstack((low, medium, high)).T

# Define fuzzy ranges for all 5 normalized features (scaled between 0â€“1)
fuzzy_ranges = {
    'voltage': [0.0, 0.5, 1.0],
    'current': [0.0, 0.5, 1.0],
    'frequency': [0.0, 0.5, 1.0],
    'temperature': [0.0, 0.5, 1.0],
    'power_factor': [0.0, 0.5, 1.0],
}

# Fuzzify each feature in the training and test set
X_train_fuzzy = []
X_test_fuzzy = []

for i, feature in enumerate(fuzzy_ranges.keys()):
    X_train_fuzzy.append(fuzzify_feature_column(X_train[:, i], fuzzy_ranges[feature]))
    X_test_fuzzy.append(fuzzify_feature_column(X_test[:, i], fuzzy_ranges[feature]))

# Concatenate all fuzzified features: each becomes 3 values (low, medium, high)
X_train_fuzzy = np.concatenate(X_train_fuzzy, axis=1)
X_test_fuzzy = np.concatenate(X_test_fuzzy, axis=1)

print("Fuzzified training input shape:", X_train_fuzzy.shape)
print("Fuzzified test input shape:", X_test_fuzzy.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Build the model
model = Sequential([
    Dense(32, input_shape=(X_train_fuzzy.shape[1],), activation='relu'),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')  # Binary classification
])

# Compile the model
model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_fuzzy, y_train, epochs=30, batch_size=16, validation_split=0.2, verbose=1)

# Evaluate
y_pred = (model.predict(X_test_fuzzy) > 0.5).astype("int32")
print(classification_report(y_test, y_pred))

# Plot accuracy
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title('Model Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

def get_model(input_dim):
    """
    Returns a new instance of the Keras ANFIS-like model.
    """
    model = Sequential([
        Dense(32, input_shape=(input_dim,), activation='relu'),
        Dropout(0.2),
        Dense(16, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def get_weights(model):
    return model.get_weights()

def set_weights(model, weights):
    model.set_weights(weights)

def average_weights(weight_list):
    avg_weights = []
    for weights in zip(*weight_list):
        avg_weights.append(np.mean(np.array(weights), axis=0))
    return avg_weights

# Recreate node datasets if not already in memory
import numpy as np
import pandas as pd

n_nodes = 5
n_samples_per_node = 200
features = ['voltage', 'current', 'frequency', 'temperature', 'power_factor']

def generate_smartgrid_data(n_samples, seed=None):
    if seed:
        np.random.seed(seed)
    data = {
        'voltage': np.random.normal(230, 10, n_samples),
        'current': np.random.normal(5, 1.5, n_samples),
        'frequency': np.random.normal(50, 1, n_samples),
        'temperature': np.random.normal(40, 5, n_samples),
        'power_factor': np.random.uniform(0.85, 1.0, n_samples),
    }
    df = pd.DataFrame(data)
    df['fault'] = np.random.choice([0, 1], size=n_samples, p=[0.9, 0.1])
    return df

# ðŸ‘‡ Run this to define `node_datasets`
node_datasets = {f'Node_{i+1}': generate_smartgrid_data(n_samples_per_node, seed=i+1) for i in range(n_nodes)}

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
import numpy as np

# Helper functions
def get_model(input_dim):
    model = Sequential([
        Dense(32, input_shape=(input_dim,), activation='relu'),
        Dropout(0.2),
        Dense(16, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def get_weights(model):
    return model.get_weights()

def set_weights(model, weights):
    model.set_weights(weights)

def average_weights(weight_list):
    avg_weights = []
    for weights in zip(*weight_list):
        avg_weights.append(np.mean(np.array(weights), axis=0))
    return avg_weights

def fuzzify_feature_column(column_data, feature_range):
    import skfuzzy as fuzz
    low = fuzz.trimf(column_data, [feature_range[0], feature_range[0], feature_range[1]])
    medium = fuzz.trimf(column_data, [feature_range[0], feature_range[1], feature_range[2]])
    high = fuzz.trimf(column_data, [feature_range[1], feature_range[2], feature_range[2]])
    return np.vstack((low, medium, high)).T

# Federated training loop
input_dim = 15
n_rounds = 5
global_model = get_model(input_dim)

for round_num in range(n_rounds):
    local_weights = []
    print(f"\nRound {round_num+1}/{n_rounds}")

    for node_name, df in node_datasets.items():
        X = df.drop('fault', axis=1).values
        y = df['fault'].values
        X_scaled = MinMaxScaler().fit_transform(X)

        fuzzy_ranges = {col: [0.0, 0.5, 1.0] for col in df.columns[:-1]}
        X_fuzzy = []
        for i, col in enumerate(fuzzy_ranges.keys()):
            X_fuzzy.append(fuzzify_feature_column(X_scaled[:, i], fuzzy_ranges[col]))
        X_fuzzy = np.concatenate(X_fuzzy, axis=1)

        X_train_local, _, y_train_local, _ = train_test_split(X_fuzzy, y, test_size=0.2, random_state=42)

        local_model = get_model(input_dim)
        set_weights(local_model, get_weights(global_model))
        local_model.fit(X_train_local, y_train_local, epochs=3, batch_size=16, verbose=0)

        local_weights.append(get_weights(local_model))

    avg_weights = average_weights(local_weights)
    set_weights(global_model, avg_weights)

# Save model
global_model.save("federated_global_model.h5")
print("âœ… Federated Global Model Saved")

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import numpy as np
import tensorflow as tf

# Reload saved global model
global_model = tf.keras.models.load_model("federated_global_model.h5")

# Prepare test data from Node_1
df = node_datasets['Node_1']
X = df.drop('fault', axis=1).values
y = df['fault'].values
X_scaled = MinMaxScaler().fit_transform(X)

# Fuzzify all features
def fuzzify_feature_column(column_data, feature_range):
    import skfuzzy as fuzz
    low = fuzz.trimf(column_data, [feature_range[0], feature_range[0], feature_range[1]])
    medium = fuzz.trimf(column_data, [feature_range[0], feature_range[1], feature_range[2]])
    high = fuzz.trimf(column_data, [feature_range[1], feature_range[2], feature_range[2]])
    return np.vstack((low, medium, high)).T

fuzzy_ranges = {col: [0.0, 0.5, 1.0] for col in df.columns[:-1]}
X_fuzzy = []
for i, col in enumerate(fuzzy_ranges.keys()):
    X_fuzzy.append(fuzzify_feature_column(X_scaled[:, i], fuzzy_ranges[col]))
X_fuzzy = np.concatenate(X_fuzzy, axis=1)

# Prediction and evaluation
y_pred = (global_model.predict(X_fuzzy) > 0.5).astype(int)

# Classification Report
print("Classification Report:\n", classification_report(y, y_pred))

# Confusion Matrix
cm = confusion_matrix(y, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["No Fault", "Fault"])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix of Federated Global Model")
plt.grid(False)
plt.show()